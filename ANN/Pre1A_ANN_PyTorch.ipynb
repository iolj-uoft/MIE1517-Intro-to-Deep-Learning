{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_7PSrzbdQbT"
      },
      "source": [
        "## Pre 1A - ANNs and PyTorch (Assignment Preparation)\n",
        "\n",
        "During the lecture we will discuss biological neurons and how we can artificially duplicate some of the processess to develop what we called an \"Artificial Piegon Brain\".\n",
        "\n",
        "We will see that these artificial neural networks (ANNs) have a relatively simple architecture defined as the \"forward path\". It is when we start considering the training and fine-tuning of the ANN's weights (parameters) to solve a specific problem that things get much more complicated.\n",
        "\n",
        "To start let us take a look at the forward pass of a simple 1-layer neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tkaXFe5jDuG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def sample_forward_pass(x, w):\n",
        "    y=[]\n",
        "    #forward pass\n",
        "    for n in range(len(x)):\n",
        "        v = 0\n",
        "        # compute w.x\n",
        "        for p in range(len(x[0])):\n",
        "            v = v + x[n][p]*w[p]\n",
        "\n",
        "        #sigmoidal activation\n",
        "        y.append(1 / (1 + math.e**(-v)))\n",
        "\n",
        "    #output model prediction\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROI2oWQpl1QJ"
      },
      "source": [
        "Make predictions on sample x. Note to keep things simple and not have to track the bias and weights separately, we make the first column as 1's. The goal here is to build a network that can make the correct predictions which are given as: 0, 0, 0, and 1 for the four samples provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82PAHW23lx7D"
      },
      "outputs": [],
      "source": [
        "# initial weights\n",
        "w = [1, -1, 1]\n",
        "\n",
        "# data (first column is for the bias term)\n",
        "x = [[1, 0.1,-0.2],\n",
        "     [1,-0.1, 0.9],\n",
        "     [1, 1.2, 0.1],\n",
        "     [1, 1.1, 1.5]]\n",
        "\n",
        "y = sample_forward_pass(x, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpjCPDqnmTPz"
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3iYm3OVmbHg"
      },
      "source": [
        "If we were lucky to select working weights then the forward pass is all that would be required in order to use our model to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULNVWRePmjU6"
      },
      "outputs": [],
      "source": [
        "# Magic weights\n",
        "\n",
        "# w = [ __ , __ , __ ]\n",
        "\n",
        "y = sample_forward_pass(x, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HdhqPjanTOc"
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk5tnC7vmrtR"
      },
      "source": [
        "The problem is that finding working weights is not trivial. To solve this we use gradient descent (i.e. continuous optimization) to learn a good set of weights. The following examples show implementatoins of gradient descent using a MSE and CrossEntropy loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vk1HIgTbbKt"
      },
      "source": [
        "#### Example: 1-layer ANN with MSE and Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tmvmSv7bSvm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# data (first column is the bias term)\n",
        "x = [[1, 0.1,-0.2],\n",
        "     [1,-0.1, 0.9],\n",
        "     [1, 1.2, 0.1],\n",
        "     [1, 1.1, 1.5]]\n",
        "\n",
        "# labels (desired output)\n",
        "t = [0, 0, 0, 1]\n",
        "\n",
        "# initial weights\n",
        "w = [1, -1, 1]\n",
        "\n",
        "iterations = 50\n",
        "learning = 10\n",
        "\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "\n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = []\n",
        "        y = []\n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            v = 0\n",
        "            # compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n][p]*w[p]\n",
        "\n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v)))\n",
        "\n",
        "            #MSE classification error\n",
        "            err.append((y[n]-t[n])**2)\n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n])*(1-y[n])*(y[n])\n",
        "                w[p] = w[p] - learning*d\n",
        "\n",
        "        #sum up classification error\n",
        "        E.append(sum(err)/len(x))\n",
        "\n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TncfaRORb5tN"
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv7VdSNxcFWT"
      },
      "source": [
        "#### Example: 1-layer ANN with Cross-Entropy and Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytnwNDitduho"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# data (first column is the bias term)\n",
        "x = [[1, 0.1,-0.2],\n",
        "     [1,-0.1, 0.9],\n",
        "     [1, 1.2, 0.1],\n",
        "     [1, 1.1, 1.5]]\n",
        "\n",
        "# labels (desired output)\n",
        "t = [0, 0, 0, 1]\n",
        "\n",
        "# initial weights\n",
        "w = [1, -1, 1]\n",
        "\n",
        "iterations = 50\n",
        "learning = 10\n",
        "\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "\n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = []\n",
        "        y = []\n",
        "\n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "            v = 0\n",
        "\n",
        "            #compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n][p]*w[p]\n",
        "\n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v)))\n",
        "\n",
        "            #cross-entropy classification error\n",
        "            err.append(-t[n]*math.log(y[n]) - (1-t[n])*math.log(1-y[n]))\n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
        "                w[p] = w[p] - learning*d\n",
        "\n",
        "        #sum up classification error\n",
        "        E.append(sum(err))\n",
        "\n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnsiF6FDdvM7"
      },
      "outputs": [],
      "source": [
        "print(y)\n",
        "print(E)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrJJw84dbiJ"
      },
      "source": [
        "### Exploring the Iris dataset\n",
        "\n",
        "Let us modify the above code to work with the iris data set.\n",
        "\n",
        "To begin, load the iris data into Google Colab. If you have difficulty with loading the data it is suggested that you use Chrome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38Qu2LlAIDlQ"
      },
      "outputs": [],
      "source": [
        "# use sklearn.datasets to load iris data\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "features, labels = load_iris(return_X_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCcmOIFyIbge"
      },
      "source": [
        "The iris data has 150 samples spread across three classes:\n",
        "1. Iris-setosa,\n",
        "2. Iris-versicolor,\n",
        "3. Iris-virginica.\n",
        "\n",
        "There are three features used:\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "\n",
        "To keep things simple, let us pick two of the classes and perform binary classification with our sample code. We will select **Iris-setosa** and **Iris-versicolor** to start:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj2vXappwiUA"
      },
      "outputs": [],
      "source": [
        "# classification iris-setosa and iris-versicolor\n",
        "import numpy as np\n",
        "\n",
        "indices = np.array(range(0,100))\n",
        "\n",
        "#setup x matrix\n",
        "x = np.zeros((len(indices), 4 + 1))\n",
        "x[:,1:5] = features[indices,:]\n",
        "\n",
        "#add bias column\n",
        "x[:,0] = np.ones(len(indices))\n",
        "\n",
        "#labels\n",
        "t = labels[indices]\n",
        "\n",
        "# initial weights\n",
        "w = np.random.rand(5)\n",
        "\n",
        "iterations = 100\n",
        "learning = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vMfTNVFJFif"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "\n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = []\n",
        "        y = []\n",
        "\n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "\n",
        "            v = 0\n",
        "            #compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n,p]*w[p]\n",
        "\n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v)))\n",
        "\n",
        "\n",
        "            #cross-entropy classification error\n",
        "            err.append(-t[n]*math.log(y[n]+ 0.000001) - (1-t[n])*math.log(1-y[n]+ 0.000001))\n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
        "                w[p] = w[p] - learning*d\n",
        "\n",
        "        #sum up classification error\n",
        "        E.append(sum(err))\n",
        "\n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgPIaMRLzjXC"
      },
      "outputs": [],
      "source": [
        "for i in range(len(t)):\n",
        "  print(t[i], y[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO8CMusz29hQ"
      },
      "source": [
        "We're able to successfully classify iris-setosa and iris-versicolor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKtAXhNZJUok"
      },
      "outputs": [],
      "source": [
        "# classification iris-versicolor and iris virginica\n",
        "import numpy as np\n",
        "\n",
        "indices = np.array(range(50,150))\n",
        "\n",
        "#setup x matrix\n",
        "x = np.zeros((len(indices), 4 + 1))\n",
        "x[:,1:5] = features[indices,:]\n",
        "\n",
        "#add bias column\n",
        "x[:,0] = np.ones(len(indices))\n",
        "\n",
        "#labels\n",
        "t = labels[indices]-1\n",
        "\n",
        "# initial weights\n",
        "w = np.random.rand(5)\n",
        "\n",
        "iterations = 100\n",
        "learning = 0.0001\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def simple_ann(x, w, t, iterations, learning):\n",
        "\n",
        "    E = []\n",
        "\n",
        "    #iterate over epochs\n",
        "    for ii in range(iterations):\n",
        "        err = []\n",
        "        y = []\n",
        "\n",
        "        #iterate over all the samples x\n",
        "        for n in range(len(x)):\n",
        "\n",
        "            v = 0\n",
        "            #compute w.x\n",
        "            for p in range(len(x[0])):\n",
        "                v = v + x[n,p]*w[p]\n",
        "\n",
        "            #sigmoidal activation\n",
        "            y.append(1 / (1 + math.e**(-v)))\n",
        "\n",
        "\n",
        "            #cross-entropy classification error\n",
        "            err.append(-t[n]*math.log(y[n]+ 0.000001) - (1-t[n])*math.log(1-y[n]+ 0.000001))\n",
        "\n",
        "            #gradient descent to compute new weights\n",
        "            for p in range(len(w)):\n",
        "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
        "                w[p] = w[p] - learning*d\n",
        "\n",
        "        #sum up classification error\n",
        "        E.append(sum(err))\n",
        "\n",
        "    return (y, w, E)\n",
        "\n",
        "(y, w, E) = simple_ann(x, w, t, iterations, learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5DlhozN2gOW"
      },
      "outputs": [],
      "source": [
        "for i in range(len(t)):\n",
        "  print(t[i], y[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iwvTlW43L8h"
      },
      "source": [
        "The performance on the iris-versicolor and iris virginica is not as good. To find out why, we will try to visualize the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb_g2BEdU_Fu"
      },
      "source": [
        "### Visualize Iris Dataset\n",
        "Since the Iris dataset has only 4 inputs we can try to visualize it on a 2-dimensional plane to get a better idea of what is happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6SleB-DVS25"
      },
      "outputs": [],
      "source": [
        "#scatter plot of iris-setosa and iris-versicolor\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "indices = np.array(range(0,100))\n",
        "\n",
        "selected_features = features[indices,:]\n",
        "selected_labels = labels[indices]\n",
        "\n",
        "feature_name = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']\n",
        "\n",
        "x_index = 0\n",
        "y_index = 1\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(selected_features[:,x_index], selected_features[:,y_index], c= selected_labels)\n",
        "plt.colorbar(ticks=[0, 1, 2])\n",
        "plt.xlabel(feature_name[x_index])\n",
        "plt.ylabel(feature_name[y_index])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcLbUFswvBCE"
      },
      "outputs": [],
      "source": [
        "# scatter plot of iris-versicolor and iris virginica\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "indices = np.array(range(50,150))\n",
        "\n",
        "selected_features = features[indices,:]\n",
        "selected_labels = labels[indices]\n",
        "\n",
        "feature_name = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm']\n",
        "\n",
        "x_index = 0\n",
        "y_index = 1\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.scatter(selected_features[:,x_index], selected_features[:,y_index], c= selected_labels)\n",
        "plt.colorbar(ticks=[0, 1, 2])\n",
        "plt.xlabel(feature_name[x_index])\n",
        "plt.ylabel(feature_name[y_index])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cqsdyg1de8M"
      },
      "source": [
        "### Nonlinear Separation\n",
        "Our 1-layer ANN was only successful on one of the binary classification combinations. A 1-layer ANN is unable to handle nonlinear separations (or decisions boundaries). To address this we can introduce a second layer known as a hidden layer. How could we do this?\n",
        "\n",
        "We can just include an additional 1-layer networks as shown in the image below.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1000/1*sX6T0Y4aa3ARh7IBS_sdqw.png)\n",
        "\n",
        "\n",
        "We would follow the same process as with the 1-layer network:\n",
        "\n",
        "1. write out the equations for the forward pass\n",
        "2. error term can stay the same MSE or Cross-Entropy\n",
        "3. Gradient descent would be applied now to two layers of weights\n",
        "\n",
        "First we would consider the forward pass for a 2-layer ANN. We could use a sigmoidal (logistic) activation function to keep things consistent with our earlier example. Note that the activation function will be applied once on the hidden layer, and also on the output layer.\n",
        "\n",
        "Computing the gradient with respect to the different layers of weights will become more difficult, but still manageable. There are just some additional terms in the chain rule. The second layer weights will be almost identical to what we computed for a 1-layer network, except the input will be the hidden layer activation.\n",
        "\n",
        "Instead of spending the time to compute gradients with each change to the network, which can be a fun mathematical exercise, we will instead focus on using the PyTorch libraries which handle all of this internally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g4umO_YStI4"
      },
      "source": [
        "### (Lab Assignment) Develop a 2-layer ANN\n",
        "Build a 2-layer network using cross-entropy. Determine the gradients with resepect to the layer 1 and layer 2 weights. How could you validate if the gradients were computed correctly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXNbhFSFTJp1"
      },
      "outputs": [],
      "source": [
        "#write code to build a 2-layer network using cross-entropy and submit as part of the lab assignment\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiwXNn3SNZfu"
      },
      "source": [
        "### What is PyTorch?\n",
        "\n",
        "PyTorch is a scientific computing package that builds on the NumPy library to numerically computer the gradients and allow for the use of GPUs. It incorporates deep learning capabilities while maximizing flexibility and speed.\n",
        "\n",
        "### PyTorch Basics\n",
        "\n",
        "To use PyTorch you must first import the library\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU7EUSNcVN9x"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTAo6LzVJwg"
      },
      "source": [
        "#### Tensors\n",
        "Tensors are n-dimensional arrays that allow that can be used with a GPU to accelerate computing. There are several ways to work with Tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MuUNQZ3UksM"
      },
      "outputs": [],
      "source": [
        "# initialize a random tensor\n",
        "x = torch.rand(4, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmcpRB48VVFe"
      },
      "outputs": [],
      "source": [
        "# initialize a tensor loaded with zeros\n",
        "x = torch.zeros(4, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI84uJM-VjyL"
      },
      "outputs": [],
      "source": [
        "# initialized with data entered manually\n",
        "x = torch.tensor([2.1, 4.0, -5.2])\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "421on8nnWE3e"
      },
      "outputs": [],
      "source": [
        "# initialized with data from numPy\n",
        "import numpy as np\n",
        "data = np.array([2.1, 4.0, -5.2])\n",
        "print(data)\n",
        "x = torch.tensor(data)\n",
        "print(x)\n",
        "\n",
        "# note you can easily convert fron tensor to numpy\n",
        "x_np = x.numpy()\n",
        "print(x_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_I8gINJWy1g"
      },
      "source": [
        "### Tensor size and shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUCzrV1nW5XA"
      },
      "outputs": [],
      "source": [
        "# obtain size of tensor data structure\n",
        "x = torch.zeros(4, 3)\n",
        "print(x.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdPIVeeEXFO5"
      },
      "source": [
        "### Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5AltppKXUZv"
      },
      "outputs": [],
      "source": [
        "# tensor addition\n",
        "x = torch.rand(4, 3)\n",
        "y = torch.ones(4, 3)\n",
        "print(x + y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UsB6YlwX_p2"
      },
      "outputs": [],
      "source": [
        "# tensor multiplication\n",
        "x = torch.rand(4, 3)\n",
        "y = torch.ones(4, 3)\n",
        "print(x * y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8W5LZlbXnMu"
      },
      "outputs": [],
      "source": [
        "# Provide output tensor as argument\n",
        "result = torch.ones(4,3)\n",
        "torch.add(x, y, out = result)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxDlMlhnYWAN"
      },
      "outputs": [],
      "source": [
        "# resize and reshape tensors\n",
        "x = torch.randn(4, 3)\n",
        "y = x.view(12)\n",
        "z = x.view(-1, 4)  # the size -1 is inferred from other dimensions\n",
        "print(x.size(), y.size(), z.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBcAFWNSZnPE"
      },
      "outputs": [],
      "source": [
        "# convert one element tensor to a Python number\n",
        "x = torch.randn(1)\n",
        "print(x)\n",
        "print(x.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhBQXibrZy0Z"
      },
      "source": [
        "### Automatic Differentiation\n",
        "\n",
        "The PyTorch autograd package allows for easy computation of derivative. This is handled automatically using a define-by-run framework, which works as you write your code.\n",
        "\n",
        "To enable this feature you need to set the Tensor attribute .required_grat to True, at which point it begins to track all operations performed on it. After your computation have been completed you can call .backward() and all the gradients will be computed for you. The gradient for each tensor will be stored in the tensor attribute .grad.\n",
        "\n",
        "Each tensor also has a .grad_fn attribute which references Function that has created it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKqBTFK2cRZW"
      },
      "outputs": [],
      "source": [
        "# Example computation of gradients\n",
        "x = torch.rand(4, 3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "#perform some operations\n",
        "y = x + 10\n",
        "z = y*y\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JDr3PgVdGMp"
      },
      "outputs": [],
      "source": [
        "# compute the gradient with respect to the output\n",
        "out.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evqmxxgjXOUd"
      },
      "source": [
        "## PyTorch Simple Neural Networks\n",
        "\n",
        "The following is an example of a 1-layer and 2-layer neural network using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdykz0ZxXbR2"
      },
      "source": [
        "PyTorch - 1-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMiAfLmxW1p1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "\n",
        "# assumes binary output\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227UvhofXll1"
      },
      "source": [
        "obtain gradients for 1-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ntotk5fW5LA"
      },
      "outputs": [],
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOtMEHf_Xj3w"
      },
      "source": [
        "PyTorch - 2-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uTVjwLkzMXr"
      },
      "outputs": [],
      "source": [
        "#2-layer neural network\n",
        "import torch\n",
        "\n",
        "num_hidden = 3\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "\n",
        "# layer 1\n",
        "w = torch.randn(5, num_hidden, requires_grad=True)\n",
        "b = torch.randn(num_hidden, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "z = torch.sigmoid(z)\n",
        "\n",
        "# layer 2\n",
        "w2 = torch.randn(num_hidden, 3, requires_grad=True)\n",
        "b2 = torch.randn(3, requires_grad=True)\n",
        "z2 = torch.matmul(z, w2)+b2\n",
        "\n",
        "# assumes binary output\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z2, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpej6hLZYGJ_"
      },
      "source": [
        "obtain gradients for 2-layer neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9WT-0L_0AZm"
      },
      "outputs": [],
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "print(w2.grad)\n",
        "print(b2.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_3rNNw5YQzq"
      },
      "source": [
        "PyTorch computational graphs make gradient calculations stright forward. Much of this is hidden away allowing you to focus more on developing and testing your model architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ntVfJhxUjtN"
      },
      "source": [
        "## Artificial Neural Networks in PyTorch\n",
        "In this example we will train an \"artificial pigeon\" to perform a digit recognition\n",
        "task. That is, we will use the MNIST dataset of hand-written digits, and train\n",
        "the pigeon to **recognize a small digit, namely a digit that is less than 3**.\n",
        "This problem is a **binary classification problem** we want to predict\n",
        "which of two classes an input image is a part of.\n",
        "\n",
        "### 1) Load MNIST Data\n",
        "The MNIST dataset contains hand-written digits that are 28x28 pixels large.\n",
        "Here are a few digits in the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOu8rvDVMpSq"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# load the data\n",
        "mnist_train = datasets.MNIST('data', train=True, download=True)\n",
        "mnist_train = list(mnist_train)[:2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3xCgqcwMvCl"
      },
      "outputs": [],
      "source": [
        "# plot the first 18 images in the training data\n",
        "for k, (image, label) in enumerate(mnist_train[:18]):\n",
        "    plt.subplot(3, 6, k+1)\n",
        "    plt.imshow(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-DcwbkZ3Nlx"
      },
      "source": [
        "### 2) Defining the ANN Forward Pass\n",
        "Here is an implementation of the artificial pigeon brain in PyTorch.\n",
        "Don't worry if this code or the explanations don't make sense yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ueh5jh3I5p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) # set the random seed\n",
        "\n",
        "class Pigeon(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pigeon, self).__init__()\n",
        "        self.layer1 = nn.Linear(28 * 28, 30)\n",
        "        self.layer2 = nn.Linear(30, 1)\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(-1, 28 * 28)\n",
        "        activation1 = self.layer1(flattened)\n",
        "        activation1 = F.relu(activation1)\n",
        "        activation2 = self.layer2(activation1)\n",
        "        return activation2\n",
        "\n",
        "pigeon = Pigeon()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi9lSSOtKhEE"
      },
      "source": [
        "In this network, there are 28x28 = 784 input neurons, to work with our 28x28 pixel images. We have a single output neuron and a hidden layer of 30 neurons.\n",
        "\n",
        "The variable `pigeon.layer1` contains information about the connectivity\n",
        "between the input layer and the hidden layer (stored as a matrix), and the\n",
        "biases (stored as a vector).\n",
        "\n",
        "Similarly, the variable `pigeon.layer2` contains information about the weights\n",
        "between the hidden layer and the output layer, and the bias.\n",
        "\n",
        "The weights and biases adjust during training, so they are called the model's\n",
        "**parameters**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82KQZkDlLx7j"
      },
      "outputs": [],
      "source": [
        "# view parameters\n",
        "for w in pigeon.layer1.parameters():\n",
        "    print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ubAvO03eMp"
      },
      "source": [
        "### 3) Test Network Forward Pass\n",
        "Here is an example of using the network to classify whether the\n",
        "image contains a small digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtiBP83X247A"
      },
      "outputs": [],
      "source": [
        "# make predictions for the first 10 images in mnist_train\n",
        "img_to_tensor = transforms.ToTensor() #transform the image data into a 28x28 matrix of numbers\n",
        "\n",
        "for k, (image, label) in enumerate(mnist_train[:10]):\n",
        "    inval = img_to_tensor(image)\n",
        "    outval = pigeon(inval)       # find the output activation given input\n",
        "    prob = torch.sigmoid(outval) # turn the activation into a probability\n",
        "    print(prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHkHYl0aO7ot"
      },
      "source": [
        "Since we haven't trained the network\n",
        "yet, the predicted probability of images containing a small digit\n",
        "is close to half. The \"pigeon\" is unsure.\n",
        "\n",
        "In order for the network to be useful, we need to actually train it, so\n",
        "that the weights are actually meaningful, non-random values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiUzssKS3ifv"
      },
      "source": [
        "### 4) Update Parameters using Gradient Descent with Cross-Entropy\n",
        "To update the parameters (weights) we will first use the network to make predictions, then compare the predictions\n",
        "agains the ground truth. To compare the predictions against actual values we'll compute a classification error using the Cross-Entropy equation.\n",
        "\n",
        "The classification error, that is how good or bad the prediction was compared to the actual values is more commonly referred to as the **loss** and Cross-Entropy is the **loss function**. The introduction of a loss function makes our problem a **optimization** problem: what set of parameters minimizes the loss across the training examples?\n",
        "\n",
        "Turning a learning problem into an optimization problem\n",
        "is actually a very subtle but important step in many machine learning tools,\n",
        "because it allows us to use tools from mathematical optimization.\n",
        "\n",
        "That there are **optimizers** that can tune the network parameters for\n",
        "us is also really, really cool. The gradient descent algorithm we derived in the last lecture is one example of an optimizer.\n",
        "\n",
        "For now, we will choose a standard loss function for a binary classification\n",
        "problem: the **binary cross-entropy loss**. We'll also choose\n",
        "a **stochastic gradient descent** optimizer. We'll talk about\n",
        "what these mean later in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZhbr5Bw270k"
      },
      "outputs": [],
      "source": [
        "# simplified training code to train `pigeon` on the \"small digit recognition\" task\n",
        "import torch.optim as optim\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(pigeon.parameters(), lr=0.005, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtHoiE6AYadE"
      },
      "source": [
        "Now, we can start to train the pigeon network, similar to the way we would train\n",
        "a real pigeon:\n",
        "\n",
        "1. We'll show the network pictures of digits, one by one\n",
        "2. We'll see what the network predicts\n",
        "3. We'll check the loss function for that example digit, comparing the network prediction against the ground truth\n",
        "4. We'll make a small update to the parameters to try and improve the loss for that digit\n",
        "5. We'll continue doing this many times -- let's say 1000 times\n",
        "\n",
        "For simplicity, we'll use 1000 images, and show the network each image only once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHmT89U0RM3C"
      },
      "outputs": [],
      "source": [
        "for (image, label) in mnist_train[:1000]:\n",
        "    # actual ground truth: is the digit less than 3?\n",
        "    actual = torch.tensor(label < 3).reshape([1,1]).type(torch.FloatTensor)\n",
        "    # pigeon prediction\n",
        "    out = pigeon(img_to_tensor(image)) # step 1-2\n",
        "    # update the parameters based on the loss\n",
        "    loss = criterion(out, actual)      # step 3\n",
        "    loss.backward()                    # step 4 (compute the updates for each parameter)\n",
        "    optimizer.step()                   # step 4 (make the updates for each parameter)\n",
        "    optimizer.zero_grad()              # a clean up step for PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfzAfZiuRQWx"
      },
      "source": [
        "It is very common to run into errors with changing different data types to tensors with the correct shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Gieq973sDh"
      },
      "source": [
        "### 5) Test Updated Network Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-RgzimS3FEW"
      },
      "outputs": [],
      "source": [
        "# make predictions for the first 10 images in mnist_train\n",
        "for k, (image, label) in enumerate(mnist_train[:10]):\n",
        "    print(label, torch.sigmoid(pigeon(img_to_tensor(image))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otc8_sYjOk1I"
      },
      "source": [
        "Not bad! We'll use the probability 50% as the cutoff for making a\n",
        "discrete prediction. Then, we can compute the accuracy on the 1000\n",
        "images we used to train the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pluL_aN30CC"
      },
      "source": [
        "### 6)a) Validation of Network on Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-Qq5Bgh4Hx9"
      },
      "outputs": [],
      "source": [
        "# computing the error and accuracy on the training set\n",
        "error = 0\n",
        "for (image, label) in mnist_train[:1000]:\n",
        "    prob = torch.sigmoid(pigeon(img_to_tensor(image)))\n",
        "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):\n",
        "        error += 1\n",
        "print(\"Training Error Rate:\", error/1000)\n",
        "print(\"Training Accuracy:\", 1 - error/1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNozPD5XOk1L"
      },
      "source": [
        "The accuracy on those 1000 images is 96%, which is really good considering\n",
        "that we only showed the network each image only once.\n",
        "\n",
        "However, this accuracy is not representative of how well the network is doing,\n",
        "because the network was *trained* on the data. The network had a chance to\n",
        "see the actual answer, and learn from that answer. To get a better sense of\n",
        "the network's predictive accuracy, we should compute accuracy numbers on\n",
        "a **test set**: a set of images that were not seen in training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VplQogR24Aue"
      },
      "source": [
        "### 6)b) Validation of Network on Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blrK9sxS4IX5"
      },
      "outputs": [],
      "source": [
        "# computing the error and accuracy on a test set\n",
        "error = 0\n",
        "for (image, label) in mnist_train[1000:2000]:\n",
        "    prob = torch.sigmoid(pigeon(img_to_tensor(image)))\n",
        "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):\n",
        "        error += 1\n",
        "print(\"Test Error Rate:\", error/1000)\n",
        "print(\"Test Accuracy:\", 1 - error/1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozLKc4pH-20V"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QjPfFMYc9r5"
      },
      "outputs": [],
      "source": [
        "# putting the above code together gives us...\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import torch.optim as optim #for gradient descent\n",
        "\n",
        "torch.manual_seed(1) # set the random seed\n",
        "\n",
        "# obtain data\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "mnist_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_data = list(mnist_data)\n",
        "mnist_train = mnist_data[:4096]\n",
        "mnist_val   = mnist_data[4096:5120]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LOeXRJlen1V"
      },
      "outputs": [],
      "source": [
        "#Artificial Neural Network Architecture (aka MLP)\n",
        "class MNISTClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 50)\n",
        "        self.fc2 = nn.Linear(50, 20)\n",
        "        self.fc3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(-1, 28 * 28)\n",
        "        activation1 = F.relu(self.fc1(flattened))\n",
        "        activation2 = F.relu(self.fc2(activation1))\n",
        "        output = self.fc3(activation2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnwzsoywdg6O"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, train=False):\n",
        "    if train:\n",
        "        data = mnist_train\n",
        "    else:\n",
        "        data = mnist_val\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=64):\n",
        "\n",
        "        output = model(imgs)\n",
        "\n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JchuI0fXdj6V"
      },
      "outputs": [],
      "source": [
        "def train(model, data, batch_size=64, num_epochs=1):\n",
        "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        for imgs, labels in iter(train_loader):\n",
        "\n",
        "            out = model(imgs)             # forward pass\n",
        "\n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "            train_acc.append(get_accuracy(model, train=True)) # compute training accuracy\n",
        "            val_acc.append(get_accuracy(model, train=False))  # compute validation accuracy\n",
        "            n += 1\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKThfucAgwvI"
      },
      "outputs": [],
      "source": [
        "model = MNISTClassifier()\n",
        "\n",
        "#proper model\n",
        "train(model, mnist_train, num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "039PX7SFZ0vm"
      },
      "source": [
        "## Enable GPU\n",
        "PyTorch allows you to run the computations on a GPU to speed up the processing. In order to enable GPUs you will need to:\n",
        "1. select GPUs in \"Notebook Settings\" found under the \"Edit\" menu option.\n",
        "2. setup model to work with the cuda\n",
        "3. make sure image and labels data are stored placed on the GPU\n",
        "\n",
        "An example of this is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUZybXVsgZ6g"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, train=False):\n",
        "    if train:\n",
        "        data = mnist_train\n",
        "    else:\n",
        "        data = mnist_val\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=64):\n",
        "\n",
        "\n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          imgs = imgs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        #############################################\n",
        "\n",
        "\n",
        "        output = model(imgs)\n",
        "\n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHdmhKvjgZ6l"
      },
      "outputs": [],
      "source": [
        "def train(model, data, batch_size=64, num_epochs=1):\n",
        "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    for epoch in range(num_epochs):\n",
        "        for imgs, labels in iter(train_loader):\n",
        "\n",
        "\n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            #############################################\n",
        "\n",
        "\n",
        "            out = model(imgs)             # forward pass\n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "            train_acc.append(get_accuracy(model, train=True)) # compute training accuracy\n",
        "            val_acc.append(get_accuracy(model, train=False))  # compute validation accuracy\n",
        "            n += 1\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUaUzEZBg4tv"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "\n",
        "model = MNISTClassifier()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "#proper model\n",
        "train(model, mnist_train, num_epochs=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}