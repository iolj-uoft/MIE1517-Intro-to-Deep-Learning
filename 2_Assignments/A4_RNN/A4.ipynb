{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bphECiUa9zw"
      },
      "source": [
        "# Assignment 4: Movie Review Analysis [50 Pt]\n",
        "\n",
        "In this assignment, we will build a recurrent neural network to work with sequential text data, specificially, movie review data to identify the reviewer sentiment. In the process of completing this assignment, you will:\n",
        "    \n",
        "1. Clean and process text data for machine learning.\n",
        "2. Perform tokenization of text data.\n",
        "3. Understand and implement a word-level recurrent neural network.\n",
        "4. Implement batching of text data using a DataLoader before training a recurrent neural network.\n",
        "5. Understand how to apply pretrained models for transfer learning in natural language processing projects.\n",
        "\n",
        "### What to submit\n",
        "\n",
        "Submit an HTML file containing all your code, outputs, and write-up\n",
        "from parts A and B. You can produce a HTML file directly from Google Colab. The Colab instructions are provided at the end of this document.\n",
        "\n",
        "Include a link to your colab file in your submission.\n",
        "\n",
        "Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission.\n",
        "\n",
        "This year we will be using an autograding script to check your model performance on a hidden test set. To do this, we need you to provide your model definition and your model weights along with your assignment submission.\n",
        "\n",
        "Instructions on the additional files you need to submit are provided below. Please make sure to test your submitted files before submitting them, failure in loading these files may result in a grade of 0 in the results section of the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“‘ **Autograding**\n",
        "\n",
        "This assignment uses an autograding script to **evaluate your model performance on a hidden test set**. You must provide your model definition and model weights along with your assignment submission to ensure compatibility with the autograding system.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ **Required Files for Submission**\n",
        "\n",
        "You need to submit the following **four files** in addition to the **HTML file** as previously instructed. Make sure to replace `your_name_connected_by_underscore` and `your_student_id` with your actual name and student ID, respectively:\n",
        "\n",
        "1. **RNN Model Definition:** `A4-RNN-your_name_connected_by_underscore-your_student_id.py`\n",
        "2. **RNN Model Weights:** `A4-RNN-your_name_connected_by_underscore-your_student_id.pth`\n",
        "3. **BERT Model Definition:** `A4-BERT-your_name_connected_by_underscore-your_student_id.py`\n",
        "4. **BERT Model Weights:** `A4-BERT-your_name_connected_by_underscore-your_student_id.pth`\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§‘â€ðŸ’» **Model Definition Files (`.py`)**\n",
        "\n",
        "### ðŸ“ **Steps to Complete:**\n",
        "\n",
        "1. **Copy Model Definition Code:** Copy the complete model definition code from your `A4.ipynb` notebook into the provided template files (`.py`).\n",
        "2. **Complete the TODOs:** Make sure all TODOs in the files are properly completed:\n",
        "   - Define your model class (`SentimentRNN` or `SentimentClassifier`).\n",
        "   - Implement the `prepare_model()` function with the exact hyperparameters used during training.\n",
        "   - Ensure preprocessing steps match those in your notebook.\n",
        "   - Set the `EMBEDDINGS_TYPE` parameter appropriately (`'pooled'` or `'last_hidden_state'` for BERT).\n",
        "3. **Command-Line Usability:** Ensure the script accepts a file path as an argument and runs from the command line as described in the file headers.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ’» **Example Command-Line Usage**\n",
        "\n",
        "```bash\n",
        "python A4-RNN-your_name_connected_by_underscore-your_student_id.py /path/to/test_dataset.csv\n",
        "```\n",
        "---\n",
        "\n",
        "\n",
        "## ðŸ’¾ **Model Weights Files (`.pth`)**\n",
        "\n",
        "1. **Save Model Weights:** During training, save your model's best-performing weights using the `torch.save()` method.\n",
        "2. **Naming Convention:** Ensure the `.pth` file names match the `.py` file names exactly, except for the file extension.\n",
        "3. **Example Code for Saving Weights:**\n",
        "\n",
        "```python\n",
        "torch.save(model.state_dict(), 'A4-RNN-your_name_connected_by_underscore-your_student_id.pth')\n",
        "```\n",
        "---\n",
        "\n",
        "## âœ… **Testing Your Submission: Autograder Compatibility**\n",
        "\n",
        "You will be provided with an **\"Example Test IMDB Dataset.csv\"** that you can use to test your autograding file compatibility. Please follow the example command line usage to provide the test data to test the script for each of the two `.py` scripts you are submitting.\n",
        "\n",
        "### ðŸ§ª **How to Test:**\n",
        "\n",
        "1. **Run the Test Notebook:** Open and run the `test_your_submitted_files.ipynb` notebook.\n",
        "2. **Verify Output:** Ensure the notebook can:\n",
        "   - Load your `.py` model definition files.\n",
        "   - Load your `.pth` model weights files.\n",
        "   - Run predictions correctly using the provided **Example Test IMDB Dataset.csv**.\n",
        "3. **Debug as Needed:** If any step fails, debug the issue and retest before submission (You can ask for help on Piazza!).\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸš¦ **Autograder Sentinel Lines**\n",
        "\n",
        "Make sure your scripts output predictions between the sentinel lines **exactly** as shown below:\n",
        "\n",
        "```text\n",
        "===start_output===\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "...\n",
        "===end_output===\n",
        "```"
      ],
      "metadata": {
        "id": "5s7OtrB7hhhE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWiUqJJTa9z6"
      },
      "source": [
        "## Colab Link\n",
        "\n",
        "Include a link to your Colab file here. If you would like the TA to look at your Colab file in case your solutions are cut off, **please make sure that your Colab file is publicly accessible at the time of submission.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "http://"
      ],
      "metadata": {
        "id": "rk7aDAaR2_wz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFMdtipUPNdu"
      },
      "source": [
        "# PART A - Sentiment Analysis\n",
        "\n",
        "In this part we will construct a world-level LSTM model for identifying positive and negative reviews. This will be done in a similar way to what was shared in the preparation code for Assignment 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgfNOUaPa9z8"
      },
      "source": [
        "# load standard modules/libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load special modules/libraries\n",
        "import os\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "from tqdm  import tqdm\n",
        "\n",
        "# load pytorch modules/libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset,DataLoader\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jLI9LBa90C"
      },
      "source": [
        "## Part 1. Data Cleaning [5 pt]\n",
        "\n",
        "We will be using the \"IMDB Movie Review Dataset\" provided on the course website. Download \"IMDB Dataset.csv\" into your Colab workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSuF7C_Ga90E"
      },
      "source": [
        "### Part (i) [1pt EXPLORATORY]\n",
        "\n",
        "Open up the file in Python, and examine some examples of positive and negative reviews. Comment on the quality of the data and any challenges you foresee in working with these data. Pick one example of a positive review and one of a negative review to support your comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_IfXHeTa90F"
      },
      "source": [
        "# download IMDB review data\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "\n",
        "# process into data and labels\n",
        "X = df['review'].values\n",
        "y = df['sentiment'].values\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8DEb5pmh07Ka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "fcIjeLLk0sE2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (ii) [1pt EXPLORATORY]\n",
        "\n",
        "Perform summary statistics on the dataset. What is the average character length of a review? What are the lengths of the longest and shortest reviews?\n",
        "\n",
        "How many positive reviews and negative reviews are there. Generate a histogram to compare the average character length for positive and negative reviews. Comment on the differences in positive and negative reviews and how that may affect the model you will be using later."
      ],
      "metadata": {
        "id": "QaDPP2Ei5k8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G5SRXZ4V6F4t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "General observations :\n",
        "\n",
        "\n",
        "Observations positive vs negative:\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "gPSFCNYI6F4u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (iii) [1pt EXPLORATORY]\n",
        "\n",
        "The following helper code will be used to process the data before we can train our LSTM model. In point form comment on what processing steps are performed in the code provided below and why these steps are necessary or beneficial to training and LSTM."
      ],
      "metadata": {
        "id": "ShOy3jp2zpRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "id": "6aamEcrqCqtU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_string(str1):\n",
        "    # remove all non-word characters excluding number and letters\n",
        "    str1= re.sub(r\"[^\\w\\s]\",'',str1)\n",
        "    # remove all whitespace with no space\n",
        "    str1= re.sub(r\"\\s\",'',str1)\n",
        "    # replace digits with no space\n",
        "    str1= re.sub(r\"\\d\",'',str1)\n",
        "    return str1\n",
        "\n",
        "def preprocess_sentence(sen1):\n",
        "    word_list=[]\n",
        "    stop_word = set(stopwords.words(\"english\"))\n",
        "    for word in sen1.lower().split():\n",
        "        word = preprocess_string(word)\n",
        "        if word not in stop_word and word!='':\n",
        "            word_list.append(word)\n",
        "    return word_list\n",
        "\n",
        "def get_stoi(data):\n",
        "    word_list=[]\n",
        "    for review in data:\n",
        "        word_list.extend(preprocess_sentence(review))\n",
        "    corpus = Counter(word_list)\n",
        "    print(corpus.get)\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_ =sorted(corpus,key= corpus.get,reverse=True)[:1000]\n",
        "    # creating a dict\n",
        "    stoi =  {ch:i+1 for i,ch in enumerate(corpus_)}\n",
        "    return stoi\n",
        "\n",
        "def tokenize(data, labels, stoi):\n",
        "    # tokenize\n",
        "    data_encoded = []\n",
        "    for review in data:\n",
        "        data_encoded.append([stoi[word] for word in preprocess_sentence(review)\n",
        "                             if word in stoi.keys()])\n",
        "\n",
        "    labels_encoded = [1 if label =='positive' else 0 for label in labels]\n",
        "\n",
        "    return np.array(data_encoded, dtype=object), np.array(labels_encoded)\n",
        "\n",
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review)!=0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "tAjcY6oPz61e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "CHDGI2rn12G_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie_D0bv9a90k"
      },
      "source": [
        "### Part (iv) [1pt EXPLORATORY]\n",
        "\n",
        "Split the dataset into `train`, `valid`, and `test`. Use a 60-20-20 split. Then apply the above processing steps to prepare your data for training.\n",
        "\n",
        "Set the padding of the reviews to 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_Y6Puz9a90l"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff5CNk7Qa90y"
      },
      "source": [
        "### Part (v) [1pt EXPLORATORY]\n",
        "\n",
        "Create a DataLoader that will allow you to load the training and validation data in mini-batches. Then generate a dataset of batch size of 16 to verify that the DataLoader works as intended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8N8qLWOa90y"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7HnqP6_a904"
      },
      "source": [
        "## Part 2. Model Building [2pt MODEL]\n",
        "\n",
        "Build a recurrent neural network model, using an architecture of your choosing. Use one or more fully-connected layers to make the prediction based on your recurrent network output.\n",
        "\n",
        "An example is provided below in `BaselineSentimentRNN`, which you can use for inspiration. However, you should build your own model.\n",
        "\n",
        "Instead of using the RNN output value for the final token, another often used strategy is to max-pool over the entire output array. That is, instead of calling something like:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(out[:, -1, :])\n",
        "```\n",
        "\n",
        "where `self.rnn` is an `nn.RNN`, `nn.GRU`, or `nn.LSTM` module, and `self.fc` is a\n",
        "fully-connected\n",
        "layer, we use:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(torch.max(out, dim=1)[0])\n",
        "```\n",
        "\n",
        "This works reasonably in practice. An even better alternative is to concatenate the max-pooling and average-pooling of the RNN outputs:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "out = torch.cat([torch.max(out, dim=1)[0],\n",
        "                 torch.mean(out, dim=1)], dim=1)\n",
        "self.fc(out)\n",
        "```\n",
        "\n",
        "We encourage you to try out all these options. The way you pool the RNN outputs is one of the \"hyperparameters\" that you can choose to tune later on."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineSentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=4, hidden_dim=4, output_dim=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        # Vanilla RNN returns: outputs, hidden\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # hidden.shape = (num_layers, batch_size, hidden_dim)\n",
        "        out = self.fc(hidden[-1])  # take the last layer's hidden state\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "WhlW4QG-38Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LTQ7zFka909"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        # TO BE COMPLETED\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "        # TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "model = SentimentRNN()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIYPl_Ba90_"
      },
      "source": [
        "## Part 3. Training [3 pt]\n",
        "\n",
        "### Part (i) [1pt MODEL]\n",
        "\n",
        "Complete the `get_accuracy` function, which will compute the\n",
        "accuracy (rate) of your model across a dataset (e.g. validation set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvNfhGD6a91A"
      },
      "source": [
        "def get_accuracy(model, data):\n",
        "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "    >>> model = MyRNN() # to be defined\n",
        "    >>> get_accuracy(model, valid_loader) # the variable `valid_loader` is from above\n",
        "    \"\"\"\n",
        "\n",
        "    # TO BE COMPLETED\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlxlcAC1a91C"
      },
      "source": [
        "### Part (ii) [1pt MODEL]\n",
        "\n",
        "Train your model. Plot the training curve of your final model.\n",
        "Your training curve should have the training/validation loss and\n",
        "accuracy plotted periodically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVtf7CJCa91D"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE3eRkDAa91F"
      },
      "source": [
        "### Part (iii) [1pt MODEL]\n",
        "\n",
        "Choose at least 4 hyperparameters to tune. Explain how you tuned the hyperparameters.\n",
        "You don't need to include your training curve for every model you trained.\n",
        "Instead, explain what hyperparemters you tuned, what the best validation accuracy was,\n",
        "and the reasoning behind the hyperparameter decisions you made.\n",
        "\n",
        "For this assignment, you should tune more than just your learning rate and epoch.\n",
        "Choose at least 2 hyperparameters that are unrelated to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2GEWfDca91G"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "mN2t6fSzryME"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gznefulsa91V"
      },
      "source": [
        "## Part 4. Evaluation [10 pt]\n",
        "\n",
        "**Important**. At this point in the assignment your test data should not have been evaluated by any of your models. The test data should be evaluated only after you have finished all the previous parts of the assignment. Once you evaluate your models on the test data you cannot change your models or else you may make hyperparameter adjustments that could lead to overfitting to the test data.\n",
        "\n",
        "### Part (i) [3pt RESULT]\n",
        "\n",
        "Report the final test accuracy of your model. Comment on how the result compares with accuracy obtained on the training and validation data. Are the results what you expected? Explain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5L5D-A1a91W"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "DTHWBJCu_Fyz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLvxrCP5Bwdu"
      },
      "source": [
        "### Part (ii) [3pt DISCUSSION]\n",
        "\n",
        "Look over the misclassified samples in the test data and see if you can find any patterns on where the model has difficulty with identifying the review sentiment. Provide up to 5 examples of positive and negative reviews each to support your findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJt0APcBBwd5"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "Fyr16MISBwd5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGHtQFpa91b"
      },
      "source": [
        "### Part (iii) [2pt RESULT]\n",
        "\n",
        "What is your model's prediction of the **probability** that\n",
        "the review message provided below is a positive review?\n",
        "\n",
        "Hint: You will need to apply the same processing on the review as was done on the the train, val, and test data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_2nSJq8a91b"
      },
      "source": [
        "review = \"\"\" Man.... I wish I loved this movie more than I did. Don't get me wrong,\n",
        "it's a solid action movie with jaw-dropping stunts (some of the best in the series),\n",
        "but as a Mission: Impossible movie, it felt like a small step backward for the franchise.\n",
        "Fallout had mind-blowing action sequences and stunt work, along with developing Ethan's\n",
        "relationship with Ilsa, providing closure with Julia, showing the lengths Ethan would\n",
        "go to protect those closest to him, and battling an imposing villain. Dead Reckoning:\n",
        "Part One stretches the movie across two films only to seemingly showcase action\n",
        "spectacle after action spectacle while sacrificing character development.\n",
        "Characters I have grown to love over a decade of films felt sidelined, ignored,\n",
        "or wasted. Hayley Atwell's new character chewed up most of the screen time, and\n",
        "while she was fantastic, I wanted to see more of the original team. The new villain\n",
        "had an inconsistent ability that confused more than intimidated. There were some\n",
        "important emotional moments that I just didn't feel the weight of when I definitely\n",
        "should have. Part Two might tie everything together and make me enjoy Part One\n",
        "more in retrospect, but unfortunately, I left wanting more from this one. \"\"\"\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CB-82EtwBmKb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD1zgYJpa91f"
      },
      "source": [
        "### Part (iv) [2pt DISCUSSION]\n",
        "\n",
        "Do you think that detecting positive and negative reviews is an easy or difficult task?\n",
        "\n",
        "Since machine learning models are expensive to train and deploy, it is very\n",
        "important to compare our models against baseline models: a simple\n",
        "model that is easy to build and inexpensive to run that we can compare our\n",
        "recurrent neural network model against.\n",
        "\n",
        "Explain how you might build a simple baseline model. This baseline model\n",
        "can be a simple neural network (with very few weights), a hand-written algorithm,\n",
        "or any other strategy that is easy to build and test.\n",
        "\n",
        "**Do not actually build a baseline model. Instead, provide instructions on\n",
        "how to build it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dwiiZYdvz34"
      },
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIR3Q5l0v1HP"
      },
      "source": [
        "# PART B - Transfer Learning\n",
        "\n",
        "For many natural language processing tasks, it is generally not a good idea to train a very large deep neural network model from scratch due to enormous compute requirements and lack of sufficient amounts of training data. Instead, you should always try to take advantage of an existing model that performs similar tasks as the one you need to solve.\n",
        "\n",
        "In this part of the assignment we will be using pretrained models to improve the performance on identifying positive and negative reviews. There are several pretrained models that are available to us, here we will use a pretrained BERT model that comes with the hugging face transformer library.\n",
        "\n",
        "Provided below is sample code to get you started. For more details please visit the hugging face tutorial on using pretrained models using PyTorch: https://huggingface.co/docs/transformers/training\n",
        "\n",
        "#### Sample Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R-_BQyiAMoo"
      },
      "source": [
        "# install relevant libraries\n",
        "!pip install -qq transformers"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "316Xs5WBv0kg"
      },
      "source": [
        "# load relevant libraries\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "################### SUBMISSION NOTE ####################################\n",
        "#  - output of this cell creates issues for converting ipynb to HTML\n",
        "#  - you may want to delete this output when you are ready to submit"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_txt = 'I want to learn how to do sentiment analysis using BERT and tokenizer.'\n",
        "\n",
        "encoding = tokenizer.encode_plus(\n",
        "  sample_txt,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        "  truncation = True\n",
        ")"
      ],
      "metadata": {
        "id": "_20pdf-tYySb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "encoding['input_ids']"
      ],
      "metadata": {
        "id": "O1B4RiOOcbGY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "encoding['attention_mask']"
      ],
      "metadata": {
        "id": "7sZ1j9PfceFF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states = bert_model(input_ids=encoding['input_ids'],\n",
        "                           attention_mask=encoding['attention_mask'])[0]\n",
        "pooled_output = bert_model(input_ids=encoding['input_ids'],\n",
        "                           attention_mask=encoding['attention_mask'])[1]"
      ],
      "metadata": {
        "id": "dZ458Q_uYzxu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = bert_model.config.hidden_size\n",
        "\n",
        "print(hidden_size)\n",
        "print(hidden_states.shape)\n",
        "print(pooled_output.shape)\n"
      ],
      "metadata": {
        "id": "GqwUAEIzZNBO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the sample code provided we loaded a short text sequence, tokenized it using the same tokenization that was used in the pretrained BERT model, and fed the tokenized input into the BERT model to obtain the embeddings.\n",
        "\n",
        "The model output consists of two forms of embeddings:\n",
        "- **hidden_states** are the final layer of outputs that has a shape sequence_length x embeddings, much like the hidden states of a recurrent neural network\n",
        "- **pooled_output** is the result of applying max pooling on the hidden states to effectively collapse the sequence dimenension and ensure the same output size for any given sequence before feeding into the classification stage\n",
        "\n",
        "Note that we preprocess all of the data prior to training a classifier stage for sentiment analysis to help speed up the training process. This is no different from the process we applied in an earlier assignment using AlexNet and image data."
      ],
      "metadata": {
        "id": "7HCaqLDaZo2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. Data Loading [5 pt]\n",
        "\n",
        "We will be using the same \"IMDB Movie Review Dataset\" that we used earlier. Reload the data and complete Part B of the assignment. You should be able to complete part B independently from Part A.\n",
        "\n",
        "### Part (i) [1pt EXPLORATORY]\n",
        "\n",
        "Provided below is a DataLoader for your training and test datasets so you can iterate over batches of data. Run the DataLoader to create your training, validation, and test data."
      ],
      "metadata": {
        "id": "4le9bqnuc2JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MovieReviewDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        reviews,\n",
        "        targets,\n",
        "        tokenizer,\n",
        "        max_len,\n",
        "        bert_model=None,\n",
        "        embed_folder='embeddings',\n",
        "        precompute=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        reviews     : array/list of text data\n",
        "        targets     : array/list of 'positive'/'negative' labels\n",
        "        tokenizer   : BERT tokenizer\n",
        "        max_len     : maximum sequence length for tokenization\n",
        "        bert_model  : a BERT model for generating embeddings (if precompute=True)\n",
        "        embed_folder: folder to store .pt files of precomputed embeddings\n",
        "        precompute  : True -> generate & save embeddings, False -> load from disk only\n",
        "        \"\"\"\n",
        "        self.reviews = reviews\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.bert_model = bert_model\n",
        "        self.embed_folder = embed_folder\n",
        "        self.precompute = precompute\n",
        "\n",
        "        # Create the folder if it doesn't exist\n",
        "        if not os.path.exists(self.embed_folder):\n",
        "            os.makedirs(self.embed_folder)\n",
        "\n",
        "        if self.precompute and (self.bert_model is not None):\n",
        "            self._precompute_embeddings()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a dictionary with:\n",
        "          - 'review_text'   : original text (optional, for reference)\n",
        "          - 'pooled_output' : [768]-dim embedding from BERT\n",
        "          - 'last_hidden'   : [seq_len, 768] from the last hidden layer\n",
        "          - 'targets'       : 0 or 1\n",
        "        \"\"\"\n",
        "        review = str(self.reviews[idx])\n",
        "        target = 1 if self.targets[idx] == 'positive' else 0\n",
        "\n",
        "        embed_path = os.path.join(self.embed_folder, f'embedding_{idx}.pt')\n",
        "        # Load precomputed embeddings\n",
        "        embedding_dict = torch.load(embed_path)\n",
        "\n",
        "        return {\n",
        "            'review_text': review,\n",
        "            'pooled_output': embedding_dict['pooled'],\n",
        "            'last_hidden': embedding_dict['last'],\n",
        "            'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "    def _precompute_embeddings(self, batch_size=256, device=\"cuda\"):\n",
        "        \"\"\"\n",
        "        Precompute embeddings in batches rather than one by one.\n",
        "        \"\"\"\n",
        "        # Move the model to device (CPU/GPU)\n",
        "        self.bert_model = self.bert_model.to(device)\n",
        "        self.bert_model.eval()\n",
        "\n",
        "        print(\"Precomputing BERT embeddings (batched)...\")\n",
        "\n",
        "        # 1) Tokenize everything\n",
        "        encodings = [self.tokenizer.encode_plus(\n",
        "            str(review),\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        ) for review in self.reviews]\n",
        "        input_ids = torch.cat([e['input_ids'] for e in encodings], dim=0)\n",
        "        attention_masks = torch.cat([e['attention_mask'] for e in encodings], dim=0)\n",
        "\n",
        "        # 2) Create a TensorDataset and DataLoader\n",
        "        dataset_tensors = TensorDataset(input_ids, attention_masks)\n",
        "        dataloader = DataLoader(dataset_tensors, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # We'll need to index back into `self.reviews` to save each sampleâ€™s .pt\n",
        "        idx_offset = 0\n",
        "\n",
        "        for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "            input_ids, attention_mask = [t.to(device) for t in batch]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    output_hidden_states=True\n",
        "                )\n",
        "            # outputs.pooler_output.shape is [batch_size, hidden_dim]\n",
        "            # outputs.hidden_states[-1].shape is [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "            pooled_output_batch = outputs.pooler_output.detach().cpu()\n",
        "            last_hidden_batch = outputs.hidden_states[-1].detach().cpu()\n",
        "\n",
        "            # 3) Save each sample in the batch\n",
        "            for i in range(len(input_ids)):\n",
        "                sample_idx = idx_offset + i\n",
        "                embed_path = os.path.join(self.embed_folder, f'embedding_{sample_idx}.pt')\n",
        "\n",
        "                # Skip if already exists (optional check)\n",
        "                if os.path.isfile(embed_path):\n",
        "                    continue\n",
        "\n",
        "                embedding_dict = {\n",
        "                    'pooled': pooled_output_batch[i].detach().cpu().clone(),\n",
        "                    'last': last_hidden_batch[i].detach().cpu().clone()\n",
        "                }\n",
        "\n",
        "                torch.save(embedding_dict, embed_path)\n",
        "\n",
        "            idx_offset += len(input_ids)\n",
        "\n",
        "        print(\"Done precomputing embeddings.\")\n"
      ],
      "metadata": {
        "id": "LzlpaXmteHQd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data loaders\n",
        "MAX_LEN = 400\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "#training data\n",
        "train_dataset = MovieReviewDataset(\n",
        "    reviews=df_train['review'].values,\n",
        "    targets=df_train['sentiment'].values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN,\n",
        "    bert_model=bert_model,       # required if we want to precompute now\n",
        "    embed_folder='train_embeds', # folder to save embeddings\n",
        "    precompute=True              # set to True so we generate them\n",
        ")\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4\n",
        ") #modify num_works as needed\n",
        "\n",
        "\n",
        "#validation data\n",
        "\n",
        "# TO BE COMPLETED\n",
        "\n",
        "#test data\n",
        "\n",
        "# TO BE COMPLETED\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "27_5roCZeI8T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (ii) [1pt EXPLORATORY]\n",
        "\n",
        "Use the **train_data_loader** to load one sample. What are the different attributes provided with the sample and how are they used?"
      ],
      "metadata": {
        "id": "g0ymyMvzfdVv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3HVP_C6JnU2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UgdwGNtik1ZR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "OOVKM8kPdPwC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (iii) [1pt EXPLORATORY]\n",
        "\n",
        "Determine the range of values for the tokens in the training data. How are the tokens obtained?\n",
        "\n",
        "Hint: You can apply your intuition here, or do some additional research to find how the \"bert-base-cased\" tokenization is done."
      ],
      "metadata": {
        "id": "GXQt2HiGfh4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6cVEH5YhkYUQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Im2KNoOPdYHU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (iv) [1pt EXPLORATORY]\n",
        "\n",
        "Generate histograms of all the token values in the training data. Repeat for the validation and test data. What are the top 5 occuring tokens in the training_dataset? What do these tokens represent?"
      ],
      "metadata": {
        "id": "6CYKglzbfTmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HTZdkR7ukhRg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "fS1c0a6wddHl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (v) [1pt EXPLORATORY]\n",
        "\n",
        "Select a single sample from your training DataLoader and feed it through the **bert_model** to obtain the hidden_states and pooled_output. Briefly describe what each tensor dimension represents and what affects the size of each dimension."
      ],
      "metadata": {
        "id": "3ElnJDdgcaiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1HOknY9Ncai9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "34OnAb9EdjtW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Model Architecture [2 pt]\n",
        "\n",
        "### Part (i) [1pt MODEL]\n",
        "\n",
        "Prepare a review classifier model that builds on the pooled output from the Bert model to identify positive and negative reviews.\n",
        "\n",
        "\n",
        "An example is provided below in `BaselineSentimentClassifierPooled`, which you can use for inspiration. However, you should build your own model."
      ],
      "metadata": {
        "id": "rDbQ7nRBek5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineSentimentClassifierPooled(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BaselineSentimentClassifierPooled, self).__init__()\n",
        "        self.linear = nn.Linear(768, n_classes)\n",
        "\n",
        "    def forward(self, pooled_embedding):\n",
        "        \"\"\"\n",
        "        pooled_embedding: shape [batch_size, 768]\n",
        "        \"\"\"\n",
        "        outputs = self.linear(pooled_embedding)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "gNDd-010GSnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifierPooled(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifierPooled, self).__init__()\n",
        "    # We don't need BERT here, since we have precomputed embeddings\n",
        "    # TO BE COMPLETED\n",
        "\n",
        "  def forward(self, pooled_embedding):\n",
        "    # TO BE COMPLETED\n",
        "\n"
      ],
      "metadata": {
        "id": "kAISbN5VexqS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (ii) [1pt MODEL]\n",
        "\n",
        "Construct the architecture for a review classifier model that uses the last hidden output from the Bert model to identify positive and negative reviews.\n",
        "\n",
        "An example is provided below in `BaselineSentimentClassifierLast`, which you can use for inspiration . However, you should build your own model.\n"
      ],
      "metadata": {
        "id": "RLFzooLUgn3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineSentimentClassifierLast(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BaselineSentimentClassifierLast, self).__init__()\n",
        "        # Again, no BERT directly needed if using precomputed data\n",
        "        self.linear = nn.Linear(768, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, last_hidden):\n",
        "        \"\"\"\n",
        "        last_hidden: shape [batch_size, seq_len, 768]\n",
        "        We'll do a simple max-pool across seq_len dimension => shape [batch_size, 768]\n",
        "        \"\"\"\n",
        "        # last_hidden has shape [B, T, 768]\n",
        "        # we want max across T => shape [B, 768]\n",
        "        x, _ = torch.max(last_hidden, dim=1)\n",
        "        x = self.dropout(x)\n",
        "        outputs = self.linear(x)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "pqDKQse6H-Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifierLast(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifierLast, self).__init__()\n",
        "    # We don't need BERT here, since we have precomputed embeddings\n",
        "    # TO BE COMPLETED\n",
        "\n",
        "  def forward(self, last_hidden):\n",
        "\n",
        "    # TO BE COMPLETED\n"
      ],
      "metadata": {
        "id": "Q34oWGhoe38H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqkKp1CgiYZb"
      },
      "source": [
        "## Part 3. Training [3 pt]\n",
        "\n",
        "### Part (i) [1pt MODEL]\n",
        "\n",
        "Complete the `get_accuracy` function, which will compute the\n",
        "accuracy (rate) of your model across a dataset (e.g. validation set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC89f2i7iYZl"
      },
      "source": [
        "def get_accuracy(model, data):\n",
        "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "    >>> model = MyRNN() # to be defined\n",
        "    >>> get_accuracy(model, valid_loader) # the variable `valid_loader` is from above\n",
        "    \"\"\"\n",
        "\n",
        "    # TO BE COMPLETED\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruaRg5JCiYZm"
      },
      "source": [
        "### Part (ii) [1pt MODEL]\n",
        "\n",
        "Write a function **train_model** to train your model. Plot the training curve of your final model.\n",
        "Your training curve should have the training/validation loss and\n",
        "accuracy plotted periodically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuqypAPxiYZm"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVvzpQRWiYZm"
      },
      "source": [
        "### Part (iii) [1pt MODEL]\n",
        "\n",
        "Choose at least 4 hyperparameters to tune. Explain how you tuned the hyperparameters. You don't need to include your training curve for every model you trained.\n",
        "Instead, explain what hyperparemters you tuned, what the best validation accuracy was,\n",
        "and the reasoning behind the hyperparameter decisions you made.\n",
        "\n",
        "For this assignment, you should tune more than just your learning rate and epoch.\n",
        "Choose at least 2 hyperparameters that are unrelated to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeCZtA9iiYZm"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "N9o-7udJmahe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4. Evaluation [10 pt]\n",
        "\n",
        "### Part (i) [3pt RESULT]\n",
        "\n",
        "Report the final test accuracy of your best BERT-based model. Then summarize in a pandas dataframe the accuracy obtained on the training, validation, and test data of your best models from Part A and B.\n",
        "\n",
        "How does the BERT model compare to the approach in part A using only LSTM? Are the results what you expected? Explain.\n",
        "\n"
      ],
      "metadata": {
        "id": "ecKaJIpQe9UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AoweZb8i2cRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "1RIZ3JWzm2a_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybdyEKx5Ww4F"
      },
      "source": [
        "### Part (ii) [2pt RESULT]\n",
        "\n",
        "Report the false positive rate and false negative rate of your model across the test set. Then summarize in a pandas dataframe the false postive and false negative rate of your model obtained on the training, validation, and test data of your best models from Part A and B.\n",
        "\n",
        "How does the BERT model compare to the approach in part A using only LSTM? Are the results what you expected? Explain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuxLCHhFWw4F"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "vIu8lB9jm3X_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "646WjsAwWw4F"
      },
      "source": [
        "### Part (iii) [3pt DISCUSSION]\n",
        "Examine some of the misclassified reviews from you best BERT and LSTM models to better identify the differences in the models. Try to provide some justification for any differences in the misclassifications observed in the models.\n",
        "\n",
        "Is there any part of the review that you could modify to make the classifications correct? Try to make small changes to the review to see if you can make the model make the correct classification while keeping the review as close to the original as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OvvnW8OWw4G"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "7TWkK5Sgm4YM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4FlSj_Mn2yY"
      },
      "source": [
        "### Part (iv) [2pt DISCUSSION]\n",
        "Find 5 samples of positive and negative reviews on IMDB that were posted recently and evaluate them with your best BERT and LSTM models from parts A and B. How well do they perform?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3DCyMW_n05f"
      },
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "0kLt9a5Xm5cO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART C (Optional) - Bonus Challenge!\n",
        "\n",
        "This is an optional exercise for those that finish the assignment early and would like to take on a challenging task.\n",
        "\n",
        "In part A we constructed and trained an LSTM model to identify the sentiment in movie reviews. In Part B we used the embeddings of a BERT model pretrained on a large corpus of text to demonstrate how transfer learning can be used to improve our movie sentiment model. The BERT model is one of many language models that we could have used to implement transfer learning.\n",
        "\n",
        "For this bonus challenge you are asked to implement a generative character-level LSTM model to produce IMDB movie reviews. Once the model is sufficiently trained you can then use its hidden states as the embedding for training a movie sentiment model. Construct your new movie sentiment analysis model and compare the performance against the model from part A and B.\n",
        "\n",
        "There are many variants of a generative LSTM model that you can consider. As a starting point you can use the generative LSTM sample code provided in the lecture notes. Specifically, the one used to generate Shakeaspeare. More advanced versions of a generative LSTM can be found in the Universal Language Model Fine-turing for Text Classification (ULMfit) paper (https://arxiv.org/abs/1801.06146).\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Create a generative character-level LSTM model trained to create IMDB reviews\n",
        "2. Create a classifier using the embeddings from the generative LSTM model (from step 1) to identify positive and negative reviews.\n",
        "3. Compare the performance of the model with the results in parts A and B of the assignment.\n",
        "4. Upgrade the generative LSTM model using the techniques listed in the ULMfit paper (e.g., bi-directional LSTM, pretraining with wikipedia text and fine-tuning on IMDBT reviews, etc.).\n",
        "\n",
        "Bonus marks will be provided based on the number of steps completed. Summarize below your results and anything intersting you learned from the steps that you completed. Bonus marks cannot be accumulated beyond a maximum assignment grade.\n"
      ],
      "metadata": {
        "id": "ARDyU7N2mWFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9eUKPfCumdpW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "ryS5uR0NB8pE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYwI4RmFS2RB"
      },
      "source": [
        "### Saving to HTML\n",
        "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
        "\n",
        "(1) download your ipynb file by clicking on File->Download.ipynb\n",
        "\n",
        "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
        "\n",
        "(3) run the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TrsqdNgS5ex"
      },
      "source": [
        "#!pip install nbconvert\n",
        "\n",
        "%%shell\n",
        "jupyter nbconvert --to html /content/A4.ipynb\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuXhlFlPTY7F"
      },
      "source": [
        "(4) the html file will be available for download in the temporary Google Colab storage\n",
        "\n",
        "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Grading Rubric\n",
        "The grading of the assignment will be based on the following categories:\n",
        "\n",
        "(1) **10 Pt - EXPLORATORY QUESTIONS** These are basic questions that in most cases can be answered without requiring a fully working and trained neural network model. For example, data loading, processing and visualization, summary statistics, data exploration, model and training setup, etc.\n",
        "\n",
        "(2) **10 Pt - MODEL** Student has successfully implemented all the required neural network models and has demonstrated successful training of the model without any errors.\n",
        "\n",
        "(3) **10 Pt - RESULT** Students are evaluated based on the results achieved in comparison to the expected results of the assignment.\n",
        "\n",
        "(4) **10 Pt - DISCUSSION QUESTIONS** Student demonstrated understanding beyond the basic exploratory questions, can answer some of the more challenging questions, and provide arguments for their model selection decisions.\n",
        "\n",
        "(5) **10 Pt - COMMUNICATION** Student has provided a quality submission that is easy to read without too many unnecessary output statements that distract the reading of the document. The code has been well commented and all the answers are communicated clearly and concisely.\n",
        "\n",
        "(6) **10 Pt - BONUS** Student has completed the assignment and has taken on the challenging bonus tasks listed in PART C. The student has demonstrated a good understanding of all aspects of the assignment and has exceeded expectations for the assignment.\n",
        "\n",
        "\n",
        "\n",
        "**TOTAL GRADE = _____ of 50 Pts**"
      ],
      "metadata": {
        "id": "Oo8xLbm4mggI"
      }
    }
  ]
}